# -*- coding: utf-8 -*-
"""evaluation.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U49jew1FW3Y7RSGplKBVlIBw9wFc7N-x
"""

import torch
import torch.nn.functional as F
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from collections import defaultdict
import matplotlib.pyplot as plt
import numpy as np


# Define a method for getting predictions from a model
def get_predictions(model, iterator, device):
    """
    Gets predictions from a model on a given data iterator.

    Args:
        model (torch.nn.Module): The trained model.
        iterator (torch.utils.data.DataLoader): Data iterator.
        device (str): Device to perform the inference on.

    Returns:
        tuple: A tuple containing images, labels, probabilities, and correctness of predictions.
    """
    model.eval()
    model.to(device)

    images = []
    labels = []
    probs = []
    corrects = []

    with torch.no_grad():
        for (x, y) in iterator:
            x = x.to(device)
            y = y.to(device)

            outputs = model(x)
            if isinstance(outputs, tuple):
                y_pred = outputs[0]
            else:
                y_pred = outputs

            y_prob = F.softmax(y_pred, dim=-1)
            _, predicted = torch.max(y_prob, 1)

            images.append(x.cpu())
            labels.append(y.cpu())
            probs.append(y_prob.cpu())

            # Determine correctness of predictions
            correct = predicted.eq(y)
            corrects.extend(correct.cpu().tolist())

    images = torch.cat(images, dim=0)
    labels = torch.cat(labels, dim=0)
    probs = torch.cat(probs, dim=0)

    return images, labels, probs, corrects

# Define a method for plotting the confusion matrix
def plot_confusion_matrix(labels, pred_labels, num_classes=38):
    """
    Plots the confusion matrix.

    Args:
        labels (torch.Tensor): True labels.
        pred_labels (torch.Tensor): Predicted labels.
        num_classes (int): Number of classes.

    Returns:
        None
    """
    fig = plt.figure(figsize=(10, 10))
    ax = fig.add_subplot(1, 1, 1)
    cm = confusion_matrix(labels, pred_labels)
    cm = ConfusionMatrixDisplay(cm, display_labels=range(num_classes))
    cm.plot(values_format='d', cmap='Blues', ax=ax)
    plt.show()

# Define a method for plotting the most incorrect predictions
def plot_most_incorrect(incorrect, n_images):
    """
    Plots the most incorrect predictions.

    Args:
        incorrect (list): List of incorrectly predicted images.
        n_images (int): Number of images to plot.

    Returns:
        None
    """
    rows = int(np.sqrt(n_images))
    cols = int(np.sqrt(n_images))

    fig = plt.figure(figsize=(20, 10))
    for i in range(rows*cols):
        ax = fig.add_subplot(rows, cols, i+1)
        image, true_label, probs = incorrect[i]
        true_prob = probs[true_label]
        incorrect_prob, incorrect_label = torch.max(probs, dim=0)

        # Normalize image tensor to [0, 1]
        image = image.permute(1, 2, 0).cpu().numpy()
        image_min, image_max = np.min(image), np.max(image)
        image = (image - image_min) / (image_max - image_min)

        ax.imshow(image, cmap='bone')
        ax.set_title(f'true label: {true_label} ({true_prob:.3f})\n'
                     f'pred label: {incorrect_label.item()} ({incorrect_prob:.3f})')
        ax.axis('off')
    fig.subplots_adjust(hspace=0.5)
    plt.show()

def count_incorrect_predictions(incorrect_examples):
    """
    Counts the occurrences of incorrect predictions for each class.

    Args:
        incorrect_examples (list): List of tuples containing incorrect examples,
                                   each tuple contains (image, true label, predicted label).

    Returns:
        dict: A dictionary where keys are class labels and values are the number of incorrect predictions for each class.
    """
    incorrect_counts = defaultdict(int)

    for _, label, _ in incorrect_examples:
        incorrect_counts[label.item()] += 1

    return incorrect_counts

def plot_incorrect_predictions_histogram(incorrect_counts, class_names):
    """
    Plots a histogram of incorrect predictions for each class.

    Args:
        incorrect_counts (dict): A dictionary where keys are class labels and values are the number of incorrect predictions for each class.
        class_names (list): A list of class names corresponding to the class labels in incorrect_counts.
    """
    plt.figure(figsize=(10, 6))
    plt.bar(class_names, incorrect_counts.values(), color='skyblue')
    plt.xlabel('Crop Disease Classes')
    plt.ylabel('Number of Incorrect Predictions')
    plt.title('Histogram of Incorrect Predictions for Each Class')
    plt.xticks(rotation=45, ha='right')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()

